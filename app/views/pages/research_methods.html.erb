<div class="pages col-md-9">
  <h1>RESEARCH METHODS</h1>
<!--   <%= image_tag 'feminist-artificial-intelligence_design-research_general.jpg', alt:'', class:'images' %> -->

  <p>Within Bits &#38; Bytes’ collaborative design research, we engaged in several generative processes,
  which includes digital/physical collage, speculative futuring, and generative video research. Additional
  research tools include the AI design toolkit, voice plugin model, pre-concept mapping and sensory walks.</p>

 <!--  <%= image_tag 'feminist-artificial-intelligence_design-research_sensory.jpg', alt:'', class:'images' %> -->

  <p>We actively designed for sound, voice, gesture, movement, touch and subjective, perceived emotion-based
  experiences over visual interfaces, and prioritize the multi-sensory experiences in our design approach to
  intelligent interfaces and agents.</p>

  <h3>AI Design Research Process</h3>

  <p>Our approach to knowledge affects the way we design for intelligence and information within intelligent systems.
  In the design workshops, we incorporated body knowledge as a basis for designing for embodiment in AI prototypes.
  Designing for embodiment, we referenced research from Sara Ahmed, Katherine Hayles and Dara Blumenthal with the
  following points in mind:</p>

  <ul>
    <li>Embodiment extends beyond the body and can include the cultural and social shaping of embodied experiences.</li>
    <li>Individual and cultural emotional experiences are subjective and embodied.</li>
    <li>Emotions, cognition, and sensations are interconnected and are often indistinguishable.</li>
    <li>The affective economies of smiling and tone of voice should be highlighted or challenged</li>
    <li>We can use our bodies and AI design to boycott the affective economies created by social spaces by addressing smile and tone.</li>
  </ul>

  <p>If emotions and embodiment extend beyond the human, how can this affect designing for embodiment in AI systems?</p>

  <h3>Sensory Socicultural Embodiment</h3>
  <h4>Facial Expressions</h4>
  <p>We studied non-verbal communication through facial expressions, specifically smiling.</p>

  <p>What is the role of a smile? On her blog Feminist Killjoys, Sara Ahmed writes about smiles.
  She highlights the connection between smiling and perceived happiness. She also highlights
  the role of smiling as a socially imposed and potentially violent behavior. In our workshops,
  we discussed the social and cultural experiences of smiling. We drew from Ahmed's main points
  and discussed the following:</p>

  <ul>
    <li>Smile as performance.</li>
    <li>Smile as employed.</li>
    <li>Smile as defense of extreme exploitation.</li>
    <li>Smiles leading to happiness.</li>
    <li>How the smile can extend beyond our faces, “We might have to turn out bodies into smiles”.</li>
  </ul>

  <p>Our discussion about the social and cultural expectations placed on women
  through the form of smiling resulted in questions like:
  <ul>
    <li>Where do you feel a smile in your body?</li>
    <li>What is the difference between an imposed smile and a genuine smile? (which in our discussion is different than a fake and real smile.)</li>
    <li>Do you want your smiles captured?</li>
  </ul>

  <p>There was an overall feeling that society imposes smiles. Workshop attendees claimed they only wanted certain
  people and designated organizations to have the ability to receive or capture their smiles.</p>

  <p>With the rise in emotions analytics employed through computer vision,
  voice analysis, body movement, what does it mean to capture a socially or
  culturally imposed smile? Our discussion leads to larger questions like:</p>
  <ul>
    <li>Can we use computer vision to understand smile types?</li>
    <li>Can pattern matching help us differentiate genuine and fake smiles, when a smile may indicate trouble or unrest or mask sadness?</li>
    <li>Could we use computer visions to understand socially and culturally imposed smiles?</li>
    <li>Is there a regional difference between real, fake and imposed smiles?</li>
    <li>How can we comabt smile detection in public? (There was a discussoin about the difference between giving a smile to a person vs. a machine.)</li>
  </ul>
<!--   <%= image_tag 'feminist-artificial-intelligence_design-research_smile.jpg', alt:'', class:'images' %>
 -->
  <p>While the discussions were critical of the smile, there was still an association between smiling and
  friendliness. The workshop conversations and generative questionnaires produced the following points:</p>

  <ul>
    <li>
      Participants associated smiling with being friendly, natural, bonding,
      and reading love and joy, an indicate of interest.
    </li>
    <li>
      Participants acknowledge that there is a difference between “real” and
      “fake” smiles.
    </li>
    <li>
      Participants reported that real smiles come from laughing, happiness,
      the eyes, inspiration, are genuine, and emerge from a feeling.
    </li>
    <li>
      Fake smiles are associated with being preoccupied, not feeling real joy,
      church, as a form of acknowledgment, to mask the feeling of
      wanting to cry, as a way to deal with someone you dislike, to be polite, and
      to engage with co-workers.
    </li>
    <li>
      Reports of situations where one feels like she has to smile include:
      when working, in professional settings, and meeting people you dislike.
    </li>

    <li>
      Respondents reported being told to smile when getting a picture taken,
      or at a social event.
    </li>

    <li>
      Participants reported the bodily location of a smile: in the cheeks,
      stomach, chest, head, and mind.
    </li>
  </ul>
  <p>

<!--   <%= image_tag 'feminist-artificial-intelligence_design-research_findings.jpg', alt:'', class:'images' %> -->
  <p>The research indicates that while the participants
  were critical of the smile in conversation, there is still a regard for the
  smile and an honoring of different types of smiles. The selective capturing of
  smiles stood out as a main point, as well as the ability to send neutral
  smiles via social media (through things like emojis or thumbs up), without
  having to smile, felt more sincere than a physically fake smile.
  </p>

<h4>TONE OF VOICE</h4>
  <p>We talked about non-verbal communication through the perceived "tone" of a voice.
  During our conversations some things emerged about the affective economy of tone.</p>

    <ul>
      <li>Participants were told to change their tone or to “get a gentler one”
      when they are considered to loud or aggressive, or complained.</li>
      <li>Participants noticed their tone changing with specific people and specific
      encounters as evidenced by “Stress makes my tone grow sharp and high pitched”,
      and “When I’m genuine, my tone is more relaxed and when I’m in formal situations
      I tend to sound hyper, like I want people to know that I’m genuinely interested in them”.</li>
    </ul>

    <p>Both vocal fry (creak voice) and upspeak rising inflection) are slowly changing the way we communicate as tone's are redistributed to us via televsion and film. Now we see these norms redistributed via commerical AI products like chatbots, Alexa, Siri, and RealDolls.</p>

    <p>This <a href="https://drive.google.com/open?id=0Bz8lvCqH2p96MHJvd0twV2dTX3M">Voice Design Toolkit</a> to further explore the relationship between the affective economy of tone
    and emerging trends in conversational user interface.

  <h4>MOVEMENT</h4>

  <p>We talked about the emotion assoicated with movement, invidually and collectively.</p>

  <ul>
  <li>A recognition that moving in a group introduced new noises and experiences.</li>
    <li>An awareness of behavior changing when entering a new environment, or encountering a person or object.
    The reports in these feelings changed significantly when the participants were blindfolded on the last
    sensory walk. When people were blindfolded they noticed the texture and slope of the ground and reported
    feeling safe when engaged in a tactile and/or spatial relationship with another person. </li>
    <li>Sensations are not considered separately, but are tied to other sensations. The parking of multiple
    sensations creates the experience. This was most obvious  when the described when sensory experiences
    contradict each other as indicated by the quote, “The smell downtown is really strong. It smells like garbage
    and stale air. The visuals contradict the smell when I look up at the big buildings. When I look down and witness
    the garbage it makes sense.” Another example of multi-sensory experiences was when a participant “notice the cold less”,
    when she saw people.</li>
  </ul>

  <h4>EMOTION</h4>

   <p>We talked about comfort our comfort level with emotions analytics, along with the role of
   emotion in our sensory-embodied experiences. We talked about the emotion assoicated with movement, invidually and collectively.</p>

  <ul>
    <li>A belief that there is a relationship between perceived mood and sensations as indicated by the workshop
    conversations and quote “My mood is definitely tied to the images I take in”.
    <li>A belief that different senses are heightened in emotional situations. This was indicated in a response
    and also discussed in the workshops, when I referenced arousal biased competition when speaking about sensation,
    perception, emotion and memory.</li>
    <li>Sensory-Emotional experiences can be triggered by external stimulus, as indicated by this response
    “Sound also is a sense that can trigger emotions for me. Music, the sound of nature, all evoke positive emotions.”</li>
    <li>There is a also a suggestions that materials and techniques are contextual and mood specific as indicated by the
    statement “As far as my personal art work, my materials and techniques change depending on my mood.” I think this
    observations speaks to the idea of contextual sensory responsiveness, while at the same time addresses the materiality of an input.</li>
  </ul>

 <h3>AI CONCEPT RESEARCH</h3>
 <h4>#1 AI MODEL MAKING</h4>

  <p>I approached questions about AI model creation across three different research areas,
  trying to connect across experiences with the unheard voices of AI. I used twitter, a
  self-created toolkit, and verbal interviews as my three forms of information collection.
  I then  took the responses from each section, put them in a google doc titled AI ideas,
  and asked participants to vote on their top three preferences. There were 33 responses.
  Rather than having a winner, I took the top four responses, because that would give the
  Bits and Bytes more resources to pull from in the AI creation. The top four responses
  came from all there categorizes, two of the four top responses came from twitter.</p>


  <h5>#1 TWITTER</h5>
   <p>The question prompted people through platform of fb and email to participate in Bits and Bytes
   twitter research. There were a series of questions asked, and the final one asked participants what
   they would like to use an AI agent for, and were requested to respond with
  #bitsbytesLA7.</p>

  <p>While I enjoy the idea of using twitter for research, there are several assumptions and challenges
  within this approach. First of all, there is an assumption that an individual users twitter and their
  settings are not set to private. Many people participated but their responses were not seen due to
  their settings. Additionally, the question itself assumes some familiarity with the concept of an AI agent.
  I do believe that for this demographic using twitter was ok, as it was an interesting way to collect data,
  and there were some interesting responses, however, twitter has decreased in popularity and would not be a
  great option for long-term research opportunities. There were several responses </p>
  <ul>
    <li>
      #bitsbytesLA7 can an AI agent turn OFF ads in the physical space (eg no more digital billboards!)?
    </li>
    <li>
      #bitsbytesLA7 I would like to use Natural Language Processing with the Anonymity software I want to build to help rape survivors.
    </li>
    <li>
     #bitsbytesLA7 An autonomous car that give me the best recommendations for my days/evening. For instance, the best shows or restaurants.
    </li>
  </ul>

  <h5>#2 AI TOOLKIT</h5>
  <p>This <a href="https://drive.google.com/open?id=0Bz8lvCqH2p96dXVRTTFDNFlJYVU">AI toolkit</a> is a downloadable PDF which anyone can download and create their own AI Model.
  The kit provides examples of a what it would mean to design with a material from, data, rulesets and
  behavior. The goal in taking a broad, less technical appraoch to this model making was to get individuals
  to think outise of traditional human-focuesed ideas about intelligence, and incoroporate other non-human
  ways of desining with AI, while at the same time tackeling social concners. While this might not be completely
  AI accurate, it was an effective tool to get people to think about altenrative ways to design. These models were
  created with women in Los Angeles and Washington DC.</p>

  <ul>
    <li>SUBVERTING SURVEILLANCE: Subverting Surveillance is a community policing model that tracks and surveils police
    to hold them accountable and prevent police brutality.</li>
    <li>HERBAL HEALTH: Plants that can be self sufficient and learn from health data using search engines to help create hybrids to help ailments.</li>
    <li>THE NEW LANGUAGE OF EMOTION: Create a new language of emotion and use it to protest (digitally and physically) throughout the city.
    The city is constructed like a quilt, nodes in a network, or spider web, all with specific points of intersection, addressing one
    small part of the whole. Each person’s individual digital emotion and environmental profile would engage differently in each location,
    utilizing physiological responses while at the same time exposing cultural bias in physical and digital spaces. This “city quilt”
    would change the profiles of people under the data quilt space, and the profiles of those under each section of quilt, could change
    that section of quilt. Outputs would be tactile, sonic and visual.</li>
  </ul>

  <h5>#3 AI AUDIO INTERVIEW HIGHLIGHTS</h5>
  <p></p>

  <ul>
    <li>BANANAS: Animal intelligence or Insect intelligence. Picking out the best bananas for a monkey based on scent.</li>
    <li>EMOTION BAROMETER: Emotion barometer / tracker on the phone. So people can know how they are communicate with people.
    Would aggregate all the data for all the different users and would categorize ranges of emotion and categorize situations
    and give recommended behavioral actions or tactics for circumstances to help improve people's behavior.</li>
    <li>LANDSCAPE PAINTINGS: Wants to use nature intelligence to make paintings.</li>
  </ul>

   <h4>#2 USING VIDEO AS MATERIAL TO UNDERSTAND DATA BAIS: VISUAL RULESETS AND BIAS MATERIALIZED IN VIDEO</h4>
   <p>The universe of the Game of Life is an infinite two-dimensional orthogonal grid of square cells,
   each of which is in one of two possible states, alive or dead, or “populated” or “unpopulated”
   (the difference may seem minor, except when viewing it as an early model of human/urban behavior
   simulation or how one views a blank space on a grid). Every cell interacts with its eight neighbors,
   which are the cells that are horizontally, vertically, or diagonally adjacent. </p>

  <p>The initial pattern constitutes the seed of the system. The first generation is created by applying
  the above rules simultaneously to every cell in the seed—births and deaths occur simultaneously, and the
  discrete moment at which this happens is sometimes called a tick. The rules continue to be applied
  repeatedly to create further generations.</p>

  <h5>Findings as materials:</h5>
  <p>18 AI models were proposed from the speech, twitter and an AI toolkit explorations. We asked the unheard voices of AI creation,
  across the United States, to contribute their thoughts about what would make an effective AI system.
  The top four votes / responses were pulled from our AI Ideas Survey. They are:</p>
  <ul>
    <li>The Emotion Barometer</li>
    <li>Subverting Surveillance</li>
    <li>Real Time Ad Remover</li>
    <li>The New Language of Emotion</li>
  </ul>

  <p>Drawing from the top 4 responses, we focused on the themes of protest, surveillance, emotion,
  communication, body data and obfuscation. The overall connection with these systems is the ability
  to use our body data to protest, monitor, obfuscate or disable systems within our environment.
  Our making used the resultes of these mdoels to focus on how we might use our body data as protest,
  creating a space for everyone to project meaning into their own body protest space.</p>

  <p>The final result included using plant information and face movment to engage in a sound portest in both
  a physical and digital space.</p>

  <h4>Processing Conway’s Game of Life</h4>
  <p>rules retrieved from Wikipedia</p>

  <ul>
    <li>At each step in time, the following transitions occur:
    <li> Any live cell with fewer than two live neighbors dies, as if caused by underpopulation.</li>
    <li>Any live cell with two or three live neighbors lives on to the next generation.</li>
    <li> Any live cell with more than three live neighbors dies, as if by overpopulation.</li>
    <li> Any dead cell with exactly three live neighbors becomes a live cell, as if by reproduction.</li>
  </ul>

  <h4>PROCESSING  Game of Life Research</h4>
  <p>What data will you use to create your rules? Here's an example:</p>

  <ul>
    <li>Take gender and salary. If salary is over $50,000 and female, fill in the cell. Play the game of life with a video. (Find a video that reflects your context). </li>
    <li>Repeat with gender reversed.</li>
    <li>Compare the two videos, open processing file, and add video. Write your ruleset in the comments.</li>
    <li>Use quick time recording to capture the process/li>
  </ul>

  <p>What might this look like using video Art?</p>
  <br>
  <iframe src="https://player.vimeo.com/video/209690366" width="858" height="480" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  <br>
  <br>
  <br>

  <iframe src="https://player.vimeo.com/video/209688118" width="858" height="480" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>
  <br>

  <p><i>Special thanks to Lee Cody for modifying these two processing scripts to achieve this visual research approach,
  and Shiveesh Fotedor for his discussions about the Game of Life.</i></p>

<br>

<!--   <%= image_tag 'feminist-artificial-intelligence_design-research_smile.jpg', alt:'', class:'images' %>
 -->
  </div>

